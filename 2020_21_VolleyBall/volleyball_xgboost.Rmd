---
title: "Predicting VolleyBall Wins with xgboost"
author: "Mike Wehinger"
date: "27/08/2021"
output:
  md_document:
    variant: markdown_github
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      warning=FALSE, 
                      message = FALSE, 
                      echo = TRUE, 
                      dpi = 180, 
                      fig.width=8, 
                      fig.height = 5)
library(tidyverse)

```

## Objective 

Following Julia Silge tutorial on how to tune an xgboost model using the 
TidyTuesday volley ball data. The model will predict wins from game play stats like errors, blocks, attacks, etc. 

### Explore Data

The data has one row per match. The games are two on two. The headers with the w_ prefix are the winning two players and the l_ prefix are the losing team. 

We will try to control for circuit, gender, and year

The data will be reshaped to better suit building the model. We will add the states for player 1 to player 2 and do so for both the winning and losing team. Then make one row per outcome (win or loss) 

*transmute() adds new variables and drops existing ones*

```{r explore}

# Load the data from tidyTuesday 
vb_matches <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-19/vb_matches.csv', guess_max = 76000)

# Add win + loss stats and disregard irrelevant columns...
vb_parsed <- vb_matches %>% transmute(
  circuit,
  gender,
  year,
  w_attacks = w_p1_tot_attacks + w_p2_tot_attacks,
  w_kills = w_p1_tot_kills + w_p2_tot_kills,
  w_erors = w_p1_tot_errors + w_p2_tot_errors,
  w_aces = w_p1_tot_aces + w_p2_tot_aces,
  w_serve_errors = w_p1_tot_serve_errors + w_p2_tot_serve_errors,
  w_blocks = w_p1_tot_blocks + w_p2_tot_blocks,
  w_digs = w_p1_tot_digs + w_p2_tot_digs,
  l_attacks = l_p1_tot_attacks + l_p2_tot_attacks,
  l_kills = l_p1_tot_kills + l_p2_tot_kills,
  l_erors = l_p1_tot_errors + l_p2_tot_errors,
  l_aces = l_p1_tot_aces + l_p2_tot_aces,
  l_serve_errors = l_p1_tot_serve_errors + l_p2_tot_serve_errors,
  l_blocks = l_p1_tot_blocks + l_p2_tot_blocks,
  l_digs = l_p1_tot_digs + l_p2_tot_digs
) %>% na.omit()

winners <- vb_parsed %>%
  select(circuit, gender, year, w_attacks:w_digs) %>%
  rename_with(~ str_remove_all(., "w_"), w_attacks:w_digs) %>%
  mutate(win = "win")

losers <- vb_parsed %>%
  select(circuit, gender, year, l_attacks:l_digs) %>%
  rename_with(~ str_remove_all(., "l_"), l_attacks:l_digs) %>%
  mutate(win = "lose")

vb_df <- rbind(winners, losers) %>% 
  mutate_if(is.character, factor)
```

This chart shows the data selected and how it might impact predictions 

```{r explorePlots}

vb_df %>%
  pivot_longer(attacks:digs, names_to = "stat", values_to = "value") %>% 
  ggplot(aes(x=gender, y=value, fill=win, color = win)) +
  geom_boxplot(alpha=.4)+
  facet_wrap(~stat, scales = "free_y", nrow=2)
  
```

### Build Model

First split data into training and testing
```{r splitData}
library(tidymodels)

# Seperate into training / testing sets...
# Use initial_time_split to preserve the order (timeseries)?

set.seed(123)
vb_split <- initial_split(vb_df, strata = win) #Stratified sampling on win/loss
vb_train <- training(vb_split) #75%
vb_test <- testing(vb_split) #25%

```

Next, build model specification (tree parameters)

*boost_tree()* defines a model that creates a series of decision trees forming an ensemble. Each tree depends on the results of previous trees. All trees in the ensemble are combined to produce a final prediction.

```{r buildMSpecs}
# no preprocessing nexessary, but a lot of model parameters to tune the model

xgb_spec <- boost_tree(
  trees =1000,
  tree_depth = tune(), min_n = tune(), loss_reduction = tune(),
  sample_size = tune(), mtry = tune(), 
  learn_rate = tune() 
) %>%
  set_engine("xgboost") %>% 
  set_mode("classification")  # you can set the more to regression or
                              # classification. This will be classification
                              #because the model will predict win or lose
  xgb_spec

```

Two approaches to setting parameters. 

1. grid_regular - set values for each tuning parameter and try every combination of those. It will take a long time 

2. grid_latin_hypercube - Space filling design. fill the six dimensional space (the tuning elements) evenly  

```{r setParameters}

#Values we are going to try...

xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(), #sample_size needs to be a proportion 
  finalize(mtry(), vb_train), # mtry contains an unknown (you need the data 
                              # to determine the value. So using the finalize()
                              # function to determine the values
  learn_rate(),
  size = 20 # 20 different models - you might want to do more 
)

xgb_grid

```

Train the 20 possible models with the parameters in xgb_grid. Using the workflow() function for convenience 

```{r workflow}

xgb_wf <- workflow() %>%
  add_formula(win ~ .) %>% # win explained by everthing else
  add_model(xgb_spec)

xgb_wf

```

get data to tune on.

```{r folds}
set.seed(123)
vb_folds <- vfold_cv(vb_train, strata = win) # 10 fold cross validation

vb_folds

```

Create / tune models

```{r model}

library(doParallel)

registerDoParallel(2) # set uo dual core processing (I need a better PC)


set.seed(234)
xgb_res <- tune_grid(
  xgb_wf, # workflow tells us what to tune
  resamples = vb_folds, # data to tune on
  grid = xgb_grid, # what parameters to try for the model
  control = control_grid(save_pred = TRUE) # save the predictions. 
)

```

### Explore Models

There are now 20 models with different parameters. Filter on the area under curve (auc) results and look at the mean vs the parameters for each model. 

This chart gives a sense of how the parameter settings work

```{r modelResults}

xgb_res %>%
  collect_metrics() %>%
  filter(.metric =="roc_auc") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry:sample_size,
               names_to = "parameter",
               values_to = "value") %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) + 
  facet_wrap(~parameter, scales = "free_x")
  

```

Pick the best model

```{r bestModel}

show_best(xgb_res, "roc_auc")

best_auc <- select_best(xgb_res, "roc_auc")

final_xgb <- finalize_workflow(xgb_wf, best_auc)

final_xgb

```

Look at variable importance 

```{r varImportance}
library(vip)

final_xgb %>%
  fit(data=vb_train) %>%
  pull_workflow_fit() %>%
  vip(geom="point")


```

Fit the final best model to the training set and evaluate the test set

```{r testSet}

final_res <- last_fit(final_xgb, vb_split)

final_res %>%
  collect_metrics()

```

Collect predictions on the test data 

```{r testPredictions}

# Confusion Matrix
final_res %>% 
  collect_predictions() %>%
  conf_mat(win, .pred_class) 

# ROC curve
final_res %>% 
  collect_predictions() %>%
  roc_curve(win, .pred_win) %>% # the truth vs probability of win
  autoplot()

```

The End